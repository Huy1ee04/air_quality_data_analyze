from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, from_unixtime, hour, to_date, year, month, dayofmonth, monotonically_increasing_id,udf, hash, round as spark_round
from pyspark.sql.types import DoubleType, DecimalType, StructType, StructField, LongType, StringType
import re
import os
from dotenv import load_dotenv
from google.cloud import bigquery, storage
from datetime import datetime,timedelta
from decimal import Decimal
import pyarrow
from pyarrow.fs import GcsFileSystem

# Load environment variables
load_dotenv()
GOOGLE_APPLICATION_CREDENTIALS = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
if not GOOGLE_APPLICATION_CREDENTIALS:
    raise ValueError("GOOGLE_APPLICATION_CREDENTIALS khÃ´ng Ä‘Æ°á»£c tÃ¬m tháº¥y! Kiá»ƒm tra láº¡i .env file.")

# Khá»Ÿi táº¡o Spark session
spark = SparkSession.builder \
    .appName("Read from GCS") \
    .config("spark.hadoop.fs.gs.impl", "com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem") \
    .config("spark.hadoop.google.cloud.auth.service.account.enable", "true") \
    .config("spark.jars", "/opt/bitnami/spark/jars/gcs-connector-hadoop3-latest.jar, /opt/bitnami/spark/jars/spark-bigquery-with-dependencies_2.12-0.36.1.jar") \
    .config("spark.hadoop.google.cloud.auth.service.account.json.keyfile", GOOGLE_APPLICATION_CREDENTIALS) \
    .config("spark.hadoop.google.cloud.project.id", "iron-envelope-455716-g8") \
    .getOrCreate()

client = storage.Client()
bucket = client.get_bucket("project-bigdata-bucket")
prefix = "air_quality_data/"
folders = set()

with open("/opt/spark/start_date.txt", "r") as f:
    start_date_str = f.read().strip()
    start_date = datetime.strptime(start_date_str, "%Y-%m-%d")

latest_date = None

# Láº¥y cÃ¡c phÃ¢n vÃ¹ng cÃ³ dáº¡ng year=YYYY/month=MM/day=DD
for blob in bucket.list_blobs(prefix=prefix):
    match = re.match(r"air_quality_data/year=(\d+)/month=(\d+)/day=(\d+)/", blob.name)
    if match:
        y, m, d = map(int, match.groups())
        folder_date = datetime(y, m, d)
        
        if folder_date >= start_date:
            folder_path = f"air_quality_data/year={y}/month={m}/day={d}"
            folders.add(folder_path)
            if latest_date is None or folder_date > latest_date:
                latest_date = folder_date

folder_paths = sorted(folders)
print(f"âœ… TÃ¬m Ä‘Æ°á»£c {len(folder_paths)} ngÃ y dá»¯ liá»‡u phÃ¢n vÃ¹ng.")


for folder in folder_paths:
    full_path = f"gs://project-bigdata-bucket/{folder}"
    print(f"ğŸš€ Äang xá»­ lÃ½ phÃ¢n vÃ¹ng: {full_path}")

    # Äá»c toÃ n bá»™ thÆ° má»¥c (khÃ´ng cáº§n láº·p tá»«ng file ná»¯a)
    df = spark.read.parquet(full_path)

    # Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
    df = df.withColumn("date", to_date(col("dt"))) \
           .withColumn("hour", hour(col("date"))) \
           .withColumn("lat", (spark_round(col("lat") / 0.25) * 0.25)) \
           .withColumn("lon", (spark_round(col("lon") / 0.25) * 0.25)) \

    # AQI UDF
    def calculate_aqi_pm25(concentration):
        # Äá»‹nh nghÄ©a cÃ¡c khoáº£ng ná»“ng Ä‘á»™ vÃ  AQI tÆ°Æ¡ng á»©ng
        breakpoints = [
            (0.0, 12.0, 0, 50),
            (12.1, 35.4, 51, 100),
            (35.5, 55.4, 101, 150),
            (55.5, 150.4, 151, 200),
            (150.5, 250.4, 201, 300),
            (250.5, 350.4, 301, 400),
            (350.5, 500.4, 401, 500)
        ]
        for bp_lo, bp_hi, i_lo, i_hi in breakpoints:
            if bp_lo <= concentration <= bp_hi:
                return ((i_hi - i_lo) / (bp_hi - bp_lo)) * (concentration - bp_lo) + i_lo
        return None

    aqi_pm25_udf = udf(calculate_aqi_pm25, DoubleType())
    df = df.withColumn("aqi", aqi_pm25_udf(df["pm2_5"]))

    # ========== Dimension Tables ==========
    dim_date = df.select("date") \
        .distinct() \
        .withColumn("year", year("date")) \
        .withColumn("month", month("date")) \
        .withColumn("day", dayofmonth("date")) \
        .withColumn("date_id", (col("year")*10000 + col("month")*100 + col("day")).cast("long"))

    dim_location = df.select("lat", "lon") \
        .distinct() \
        .withColumn("location_id", hash("lat", "lon"))

    # ========== Fact Table ==========
    df_with_date_id = df.join(dim_date, on="date", how="left")
    df_with_ids = df_with_date_id.join(dim_location, on=["lat", "lon"], how="left")

    fact_air_quality = df_with_ids.select(
        "date_id", "hour", "location_id", "pm2_5", "pm10", "o3", "so2", "no2", "co", "aqi", "aqi_level"
    ).withColumn("fact_id", hash("date_id", "hour", "location_id"))

    daily_avg_df = fact_air_quality.groupBy("date_id", "location_id").agg(
        avg("aqi").alias("daily_avg_aqi"),
        avg("pm2_5").alias("daily_avg_pm2_5"),
        avg("pm10").alias("daily_avg_pm10"),
        avg("o3").alias("daily_avg_o3"),
        avg("so2").alias("daily_avg_so2"),
        avg("no2").alias("daily_avg_no2"),
        avg("co").alias("daily_avg_co")
    ).orderBy("date_id", "location_id")


    # ==================== Hiá»ƒn thá»‹ ====================
    # print("âœ… Dimension Date:")
    # dim_date.show()
    # print("âœ… Dimension Location:")
    # dim_location.show()
    # print("âœ… Fact Table:")
    # fact_air_quality.show()
    # print("âœ… Avg per day Table:")
    # daily_avg_df.show()

    # ==================== Tuá»³ chá»n: Ghi ra GCS ====================
    # dim_date.write.mode("overwrite").parquet("gs://project-bigdata-bucket/star/dim_date")
    # dim_location.write.mode("overwrite").parquet("gs://project-bigdata-bucket/star/dim_location")
    # fact_air_quality.write.mode("overwrite").parquet("gs://project-bigdata-bucket/star/fact_air_quality")
    # daily_avg_aqi_df.write.mode("overwrite").parquet("gs://project-bigdata-bucket/star/daily_avg_aqi")

    # ==================== Ghi ra BigQuery ====================
    dim_date.write.format("bigquery") \
        .option("table", "iron-envelope-455716-g8.aq_data.dim_date") \
        .option("parentProject", "iron-envelope-455716-g8") \
        .option("temporaryGcsBucket", "project-bigdata-bucket") \
        .mode("append") \
        .save()

    dim_location.write.format("bigquery") \
        .option("table", "iron-envelope-455716-g8.aq_data.dim_location") \
        .option("parentProject", "iron-envelope-455716-g8") \
        .option("temporaryGcsBucket", "project-bigdata-bucket") \
        .mode("append") \
        .save()

    fact_air_quality.write.format("bigquery") \
        .option("table", "iron-envelope-455716-g8.aq_data.fact_air_quality") \
        .option("parentProject", "iron-envelope-455716-g8") \
        .option("temporaryGcsBucket", "project-bigdata-bucket") \
        .mode("append") \
        .save()

    daily_avg_df.write.format("bigquery") \
        .option("table", "iron-envelope-455716-g8.aq_data.daily_avg") \
        .option("parentProject", "iron-envelope-455716-g8") \
        .option("temporaryGcsBucket", "project-bigdata-bucket") \
        .mode("append") \
        .save()

print("âœ… ÄÃ£ ghi dá»¯ liá»‡u vÃ o BigQuery thÃ nh cÃ´ng!")

# Sau vÃ²ng láº·p, lÆ°u ngÃ y káº¿ tiáº¿p cá»§a latest_date vÃ o file náº¿u cÃ³
if latest_date:
    next_date = latest_date + timedelta(days=1)
    with open("/opt/spark/start_date.txt", "w") as f:
        f.write(next_date.strftime("%Y-%m-%d"))
    print(f"âœ… ÄÃ£ lÆ°u ngÃ y tiáº¿p theo: {next_date.strftime('%Y-%m-%d')}")
else:
    print("âš ï¸ KhÃ´ng cÃ³ ngÃ y nÃ o phÃ¹ há»£p Ä‘á»ƒ cáº­p nháº­t.")